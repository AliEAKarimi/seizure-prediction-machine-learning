{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats as st\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "import os\n",
    "\n",
    "seed = 57\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "x = pickle.load(open('x.pkl', 'rb'))\n",
    "y = pickle.load(open('y.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normal = np.concatenate((x[:300], x[400:]), axis=0)\n",
    "print(x_normal.shape)\n",
    "x_seizure = x[300:400]\n",
    "print(x_seizure.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_freq = 173.6 #based on info from website\n",
    "\n",
    "b, a = butter(3, [0.5,40], btype='bandpass',fs=sampling_freq)\n",
    "\n",
    "x_normal_filtered = np.array([lfilter(b,a,x_normal[ind,:]) for ind in range(x_normal.shape[0])]);\n",
    "print(x_normal_filtered.shape)\n",
    "x_seizure_filtered = np.array([lfilter(b,a,x_seizure[ind,:]) for ind in range(x_seizure.shape[0])]);\n",
    "print(x_seizure_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normal = x_normal_filtered\n",
    "x_seizure = x_seizure_filtered\n",
    "\n",
    "x = np.concatenate((x_normal,x_seizure))\n",
    "y = np.concatenate((np.zeros((400,1)),np.ones((100,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the data\n",
    "def plot(x, title):\n",
    "    plt.plot(x)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Define a function to evaluate the model\n",
    "def evaluation(test, pred):\n",
    "    print('Accuracy: ', accuracy_score(test, pred))\n",
    "    print('Recall: ', recall_score(test, pred))\n",
    "    print('Precision: ', precision_score(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Feature Extraction #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical features: mean, std, max, min, median, variance, skewness, kurtosis, mode;\n",
    "mean = np.mean(x, axis=1)\n",
    "plot(mean, 'mean')\n",
    "\n",
    "std = np.std(x, axis=1)\n",
    "plot(std, 'std')\n",
    "\n",
    "max = np.max(x, axis=1)\n",
    "plot(max, 'max')\n",
    "\n",
    "min = np.min(x, axis=1)\n",
    "plot(min, 'min')\n",
    "\n",
    "median = np.median(x, axis=1)\n",
    "plot(median, 'median')\n",
    "\n",
    "var = np.var(x, axis=1)\n",
    "plot(var, 'var')\n",
    "\n",
    "skewness = st.skew(x, axis=1)\n",
    "plot(skewness, 'skewness')\n",
    "\n",
    "kurtosis = st.kurtosis(x, axis=1)\n",
    "plot(kurtosis, 'kurtosis')\n",
    "\n",
    "# the output of mode is exactly like the min of our data, so we ignore it.\n",
    "mode = []\n",
    "for row in x:\n",
    "    mode.append(st.mode(row, keepdims=True)[0][0])\n",
    "mode = np.array(mode)\n",
    "plot(mode, 'mode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time domain features: mobility, complexity, average absolute signal slope, peak-to-peak\n",
    "def hjorth_params(x, axis=-1):\n",
    "    x = np.asarray(x)\n",
    "    # Calculate derivatives\n",
    "    dx = np.diff(x, axis=axis)\n",
    "    ddx = np.diff(dx, axis=axis)\n",
    "    # Calculate variance\n",
    "    x_var = np.var(x, axis=axis)  # = activity\n",
    "    dx_var = np.var(dx, axis=axis)\n",
    "    ddx_var = np.var(ddx, axis=axis)\n",
    "    # Mobility and complexity\n",
    "    mob = np.sqrt(dx_var / x_var)\n",
    "    com = np.sqrt(ddx_var / dx_var) / mob\n",
    "    return mob, com\n",
    "\n",
    "mobility, complexity = hjorth_params(x)\n",
    "plot(mobility, 'mobility')\n",
    "plot(complexity, 'complexity')\n",
    "\n",
    "\n",
    "peak_to_peak = np.ptp(x, axis=1)\n",
    "plot(peak_to_peak, 'peak_to_peak')\n",
    "\n",
    "average_absolute_signal_slope = np.mean(np.abs(np.diff(x, axis=1)), axis=1)\n",
    "plot(average_absolute_signal_slope, 'average_absolute_signal_slope')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency domain features: Delta, Theta, Alpha, Beta, Gamma\n",
    "def bandpower(data, sf, band, window_sec=None, relative=False):\n",
    "    \"\"\"Compute the average power of the signal x in a specific frequency band.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : 1d-array\n",
    "        Input signal in the time-domain.\n",
    "    sf : float\n",
    "        Sampling frequency of the data.\n",
    "    band : list\n",
    "        Lower and upper frequencies of the band of interest.\n",
    "    window_sec : float\n",
    "        Length of each window in seconds.\n",
    "        If window_sec=None, window_sec = (1 / min(band)) * 2\n",
    "    relative : bool\n",
    "        If relative is True, return the relative power (= divided by the total power of the signal).\n",
    "    Returns\n",
    "    -------\n",
    "    bp : float\n",
    "        Absolute or relative band power.\n",
    "    \"\"\"\n",
    "    from scipy.signal import welch\n",
    "    from scipy.integrate import simps\n",
    "\n",
    "    band = np.asarray(band)\n",
    "    low, high = band\n",
    "\n",
    "    # Define window length\n",
    "    if window_sec is not None:\n",
    "        nperseg = window_sec * sf\n",
    "    else:\n",
    "        nperseg = (2 / low) * sf\n",
    "\n",
    "    # Compute the modified periodogram (Welch)\n",
    "    freqs, psd = welch(data, sf, nperseg=nperseg)\n",
    "\n",
    "    # Frequency resolution\n",
    "    freq_res = freqs[1] - freqs[0]\n",
    "\n",
    "    # Find closest indices of band in frequency vector\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "\n",
    "    # Integral approximation of the spectrum using Simpson's rule.\n",
    "    bp = simps(psd[idx_band], dx=freq_res)\n",
    "\n",
    "    if relative:\n",
    "        bp /= simps(psd, dx=freq_res)\n",
    "    return bp\n",
    "\n",
    "delta = []\n",
    "theta = []\n",
    "alpha = []\n",
    "beta = []\n",
    "gamma = []\n",
    "\n",
    "for row in x:\n",
    "    delta.append(bandpower(row, 128, [0.5, 4], 4))\n",
    "    theta.append(bandpower(row, 128, [4, 8], 4))\n",
    "    alpha.append(bandpower(row, 128, [8, 13], 4))\n",
    "    beta.append(bandpower(row, 128, [13, 30], 4))\n",
    "    gamma.append(bandpower(row, 128, [30, 50], 4))\n",
    "delta = np.array(delta)\n",
    "theta = np.array(theta)\n",
    "alpha = np.array(alpha)\n",
    "beta = np.array(beta)\n",
    "gamma = np.array(gamma)\n",
    "plot(delta, 'delta')\n",
    "plot(theta, 'theta')\n",
    "plot(alpha, 'alpha')\n",
    "plot(beta, 'beta')\n",
    "plot(gamma, 'gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing data dimensions using extracted features\n",
    "x_visualized = np.array([mean, std, max, min, median, var, skewness, kurtosis, peak_to_peak, average_absolute_signal_slope, delta, theta, alpha, beta, gamma])\n",
    "x_visualized = x_visualized.T\n",
    "print(x_visualized.shape)\n",
    "plot(x_visualized, 'X Visualized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### Classification #######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using train_test_split to split data into train and test\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(x,y,random_state=seed,test_size=0.2)\n",
    "print(xx_train.shape)\n",
    "print(xx_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using train_test_split to split visualized data into train and test\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_visualized,y,random_state=seed,test_size=0.2)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=seed, shuffle=True)\n",
    "\n",
    "def cross_validation(clf, x, y):\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'recall': make_scorer(recall_score),\n",
    "        'precision': make_scorer(precision_score),\n",
    "    }\n",
    "    scores = cross_validate(clf, x, y, cv=kf, scoring=scoring)\n",
    "    print(scores)\n",
    "    print('Accuracy: ', np.mean(scores['test_accuracy']))\n",
    "    print('Recall: ', np.mean(scores['test_recall']))\n",
    "    print('Precision: ', np.mean(scores['test_precision']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with linear kernel that used of train_test_split for splitting data\n",
    "linear_svm_clf = SVC(kernel='linear')\n",
    "linear_svm_clf.fit(xx_train, yy_train)\n",
    "svm_y_pred = linear_svm_clf.predict(xx_test)\n",
    "evaluation(yy_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with linear kernel that used of train_test_split for splitting visualized data\n",
    "linear_svm_clf.fit(x_train, y_train)\n",
    "svm_y_pred = linear_svm_clf.predict(x_test)\n",
    "evaluation(y_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with linear kernel that used of cross validation(k_fold) for splitting data\n",
    "cross_validation(linear_svm_clf, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with linear kernel that used of cross validation(k_fold) for splitting visualized data\n",
    "cross_validation(linear_svm_clf, x_visualized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with rbf kernel that used of train_test_split for splitting data\n",
    "rbf_svm_clf = SVC(kernel='rbf')\n",
    "rbf_svm_clf.fit(xx_train, yy_train)\n",
    "svm_y_pred = rbf_svm_clf.predict(xx_test)\n",
    "evaluation(yy_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with rbf kernel that used of train_test_split for splitting visualized data\n",
    "rbf_svm_clf.fit(x_train, y_train)\n",
    "svm_y_pred = rbf_svm_clf.predict(x_test)\n",
    "evaluation(y_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with rbf kernel that used of cross validation(k_fold) for splitting data\n",
    "cross_validation(rbf_svm_clf, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with rbf kernel that used of cross validation(k_fold) for splitting visualized data\n",
    "cross_validation(rbf_svm_clf, x_visualized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with poly kernel that used of train_test_split for splitting data\n",
    "poly_svm_clf = SVC(kernel='poly')\n",
    "poly_svm_clf.fit(xx_train, yy_train)\n",
    "svm_y_pred = poly_svm_clf.predict(xx_test)\n",
    "evaluation(yy_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with poly kernel that used of train_test_split for splitting visualized data\n",
    "poly_svm_clf.fit(x_train, y_train)\n",
    "svm_y_pred = poly_svm_clf.predict(x_test)\n",
    "evaluation(y_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with poly kernel that used of cross validation(k_fold) for splitting data\n",
    "cross_validation(poly_svm_clf, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with poly kernel that used of cross validation(k_fold) for splitting visualized data\n",
    "cross_validation(poly_svm_clf, x_visualized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with sigmoid kernel that used of train_test_split for splitting data\n",
    "sigmoid_svm_clf = SVC(kernel='sigmoid')\n",
    "sigmoid_svm_clf.fit(xx_train, yy_train)\n",
    "svm_y_pred = sigmoid_svm_clf.predict(xx_test)\n",
    "evaluation(yy_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with sigmoid kernel that used of train_test_split for splitting visualized data\n",
    "sigmoid_svm_clf.fit(x_train, y_train)\n",
    "svm_y_pred = sigmoid_svm_clf.predict(x_test)\n",
    "evaluation(y_test,svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with sigmoid kernel that used of cross validation(k_fold) for splitting data;\n",
    "cross_validation(sigmoid_svm_clf, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm with sigmoid kernel that used of cross validation(k_fold) for splitting data;\n",
    "cross_validation(sigmoid_svm_clf, x_visualized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest that used of train_test_split for splitting data\n",
    "random_forest_clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=seed)\n",
    "random_forest_clf.fit(xx_train, yy_train)\n",
    "random_forest_y_pred = random_forest_clf.predict(xx_test)\n",
    "evaluation(yy_test,random_forest_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest that used of train_test_split for splitting visualized data\n",
    "random_forest_clf.fit(x_train, y_train)\n",
    "random_forest_y_pred = random_forest_clf.predict(x_test)\n",
    "evaluation(y_test,random_forest_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest that used of cross validation(k_fold) for splitting data;\n",
    "cross_validation(random_forest_clf, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest that used of cross validation(k_fold) for splitting visualized data;\n",
    "cross_validation(random_forest_clf, x_visualized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn that used of train_test_split for splitting data\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_clf.fit(xx_train, yy_train)\n",
    "knn_y_pred = knn_clf.predict(xx_test)\n",
    "evaluation(yy_test,knn_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn that used of train_test_split for splitting visualized data\n",
    "knn_clf.fit(x_train, y_train)\n",
    "knn_y_pred = knn_clf.predict(x_test)\n",
    "evaluation(y_test,knn_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn that used of cross validation(k_fold) for splitting data;\n",
    "cross_validation(knn_clf, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn that used of cross validation(k_fold) for splitting visualized data;\n",
    "cross_validation(knn_clf, x_visualized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing ROC curve\n",
    "y_score = random_forest_clf.predict_proba(x_test)[::,1]\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(y_test,  y_score)\n",
    "auc = roc_auc_score(y_test, y_score)\n",
    "plt.plot(false_positive_rate,true_positive_rate,label=\"auc=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing confusion matrix\n",
    "cm = confusion_matrix(y_test, random_forest_y_pred, normalize='true')\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[False, True])\n",
    "cm_display.plot()\n",
    "plt.title('Confusion matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bde601f9cc4e066d7577ad32a850469c062c211b878408250eeeeff722c10757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
